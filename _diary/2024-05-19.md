#Kafka #Message-Broker

### # 9개 프로젝트로 경험하는 대용량 트래픽 & 데이터 처리 초격차 패키지 Online - 패스트 캠퍼스

#### 1. Kafka Basic
##### Overview
* Distributed ~~Message~~/Event Broker
* Pub/Sub, Multiple Read
* Partition, Replica
* Components
	* Producer : Key (for Partition)
	* Consumer/Consumer Group : Offset, Lag, Commit, Pull (backpresure)
	* Topic : Partition by Key (Shard)
	* Broker
		* Server Process
		* Partition Leader/Follower, Controller (leader election)
		* In Sync Replica (by Commit Lag of Follower, Candidate Leader Group)
	* Zookeeper
		* Metadata
		* Offset/Partition Leader
##### vs RabbitMQ

|              | RabbitMQ                                                         | Kafka                                             |
| ------------ | ---------------------------------------------------------------- | ------------------------------------------------- |
| Mesage Florw | Producer<br>-> Exchange -> Binding Rules -> Queue<br>-> Consumer | Producer<br>-> Broker -> Partition<br>-> Consumer |
|              | Easy, Flexible                                                   | Large Throughput, Scaling, Reliability            |
| Message      | Push                                                             | Pull                                              |
| Throughput   | 10k/sec                                                          | 1 million/sec                                     |
|              | Transactional                                                    | Operational                                       |

#### 2. Kafka Setup
##### Install

```bash
apt-get install default-jre # Java 11
vi ~/.bashrc
# export JAVA_HOME="..."

mkdir kafka && cd kafka
curl "https://archive.apahge.org/dist/kafka/2.7.2/kafka_2.12-2.7.2.tgz" -o kafka.tgz
tar -xvzf kafka.tgz --strip 1
```

```bash
~/kafka/bin/kafka-server-start.sh
# KAFKA_LOG4J_OPTS="...=file:$base_dir/../config/log4j.properties"
# KAFKA_HEAP_OPS="-Xmx1G -Xms1G"
```
##### Configure

```bash
mkdir ~/kafka/zookeeper
echo "$myid" > ~/kafka/zookeeper/myid # myid: 1,2,3

vi ~/kafka/config/zookeeper.properties
# dataDir/~/kafka/zookeeper
#
# server.1=<ip-zookeeper-1>:2888:3888
# server.2=<ip-zookeeper-1>:2888:3888
# server.3=<ip-zookeeper-1>:2888:3888

vi ~/kafka/cofnig/server.peoperties
# broker.id=<my-id> # 0,1,2
#
# zookeeper.connect=<ip-zookeeper-1>:2181,<ip-zookeeper-2>:2181,<ip-zookeeper-3>:2181
#
# advertised.listeners=PLAINTEXT://<public-ip>:9092
#
# log.dirs=~/kafka/logs

rm -rf ~/kafka/logs/meta.properties
```
##### Register Service

```bash
vi /etc/systemd/system/zookeeper.service
# [Unit]
# Requires=...
# After=...
#
# [Service]
# Type=...
# User=...
# ExecStart=...
# ExecStop=...
# Restart=...
#
# [Install]
# WantedBy=...

vi /etc/systemd/system/kafka.service
# ...

sudo sustemctl enable zookeeper
sudo sustemctl enable kafka
```

```bash
service kafka status
service kafka start
service kafka stop
service kafka restart
```
#### 3. Kafka Command

```bash
~/kafka/bin/kafka-console-consumer.sh \
  --bootstrap-server broker1:9092,... \
  --topic my-topic \
  --from-beginnig

~/kafka/bin/kafka-console-producer.sh \
  --broker-list broker1:9092,... \
  --topic my-topic

~/kafka/bin/kafka-console-producer.sh \
  --broker-list broker1:9092,... \
  --property "parse.key=true" --property "key.separator=:" \
  --topic my-topic

~/kafka/bin/kafka-topics.sh \
  --zookeeper zookeeper1:2181, ... \
  --partitions 3 \
  --replication-factor 1 \
  --topic my-topic \
  --create
```

#### 4. Spring Kafka

**Configuration**
* Producer
	* `KafkaTemplate<K,V>(ProducerFactory)`
	* `ProducerFactory<K,V>`, `DefaultKafkaProducerFactory<>(configMap)`
	* `configMap: Map<String,String>`
		* `ProducerConfig.BOOTSTRAP_SERVER_CONFIG`=`broker1:9092,broker2:9092,broker3:9092`
		* `ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG`=`StringSerializer.class`
		* `ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG`=`StringSerializer.class`
* Consumer
	* `ConcurrentKafkaListenerContainerFactory<K,V>(ConsumerFactory)`
	* `ConsumerFactory<K,V>(configMap)`, `DefaultKafkaConsumerFactory<>(configMap)`
	* `configMap: Map<String,String>`
		* `ConsumerConfig.BOOTSTRAP_SERVER_CONFIG`=`broker1:9092,broker2:9092,broker3:9092`
		* `ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG`=`StringDeserializer.class`
		* `ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG`=`StringDeserializer.class`

* `KafkaTemplate`
	* `send(topic: String, value: Object)`
	* `send(topic: String, key: String, value: Object)`
	* `send(topic: String, partition: Integer, key: String, value: Object)`
* `@KafkaListener(topic = String, groupId = String)`
	* `void consume(String value) { ... }`

#### 5. Kafka Streams

##### Configuration
* `KafkaStreamsConfiguration`
  `KafkaStreamsDefaultConfiguration.DEFAULT_STREAMS_CONFIG_BEAN_NAME`
* `configMap: Map<String,String>`
	* `StreamsConfig.APPLICATIN_ID_CONFIG`=`<app-id>`
	* `StreamsConfig.BOOTSTRAP_SERVER_CONFIG`=`broker1:9092,broker2:9092,broker3:9092`
	* `StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG`=`Serdes.String()`
	* `StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG`=`Serdes.String()`
	* `StreamsConfig.NUM_STREAM_THREADS_CONFIG`=`3`

```java
Map<String,String> configMap = Map.of(
	StreamsConfig.APPLICATIN_ID_CONFIG, "<app-id>",
	StreamsConfig.BOOTSTRAP_SERVER_CONFIG, "broker1:9092,broker2:9092,broker3:9092",
	StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String(),
	StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String(),
	StreamsConfig.NUM_STREAM_THREADS_CONFIG, 3
)
```

##### KStream

```java
StreamsBuilder.stream(
	"consume-topic",
	Consumed.with(
		STRING_SERDE,
		STRING_SERDE
	)
) : KStream<K,V>

KStream<K,V>
	.print(Printed.toSysOut())
	.filter((key, val) -> val...)
	.to("producer-topic")
```

Streams Join
* https://www.confluent.io/blog/crossing-streams-joins-apache-kafka/

```java
/*
 *        leftStream = 1:left
 *       rightStream = 1:right
 *      joinedStream = 1:left:1:right
 * outerJoinedStream = 
 */
var keyValueMapper = (key, val) -> v.split(":")[0] // key:value
KStream<K,V> left = StreamBuilder.stream(...).selectKey(keyValueMapper)
KStream<K,V> right = StreamBuilder.stream(...).selectKey(keyValueMapper)

var valueJoiner = (leftV, rightV) -> leftV + ":" + rightV // BiFunction<V1,V2,R>
var joinWindow = JoinWindows.ofTimeDifferenceWithNoGrace(Duration.ofMinutes(1))
left.join(right, valueJoiner, joinWindow)
	.print(Printed.toSysOut())
	.to("producer-topic") // when extract matched key (in time window)

left.leftJoin(right, valueJoiner, joinWindow)
	.to("producer-topic") // when left value + inner join (in time window)

left.outerJoin(right, valueJoiner, joinWindow)
	.to("producer-topic") // when left/right value + inner join (in time window)
```